"""
æµ‹è¯• Ollama Cloud é›†æˆ
è¿è¡Œæ–¹å¼: python test_ollama.py
"""

import os
import sys
import asyncio
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ£€æŸ¥ API Key
api_key = os.getenv("OLLAMA_API_KEY")
if not api_key or api_key == "your-ollama-api-key-here":
    print("âŒ é”™è¯¯: è¯·å…ˆåœ¨ .env æ–‡ä»¶ä¸­é…ç½® OLLAMA_API_KEY")
    print("   è·å–æ–¹å¼: https://ollama.com/settings/keys")
    sys.exit(1)

# è®¾ç½® OLLAMA_API_KEY ç¯å¢ƒå˜é‡ (langchain_ollama ä¼šè‡ªåŠ¨è¯»å–)
os.environ["OLLAMA_API_KEY"] = api_key

print(f"âœ… API Key å·²é…ç½®: {api_key[:15]}...")

# æµ‹è¯•å¯¼å…¥
try:
    from langchain_ollama import ChatOllama
    from langchain_core.messages import HumanMessage, SystemMessage
    print("âœ… ä¾èµ–åŒ…å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ä¾èµ–åŒ…å¯¼å…¥å¤±è´¥: {e}")
    print("   è¯·è¿è¡Œ: pip install langchain-ollama langchain-core ollama")
    sys.exit(1)

# æµ‹è¯• Ollama è¿æ¥
async def test_ollama():
    print("\nğŸš€ å¼€å§‹æµ‹è¯• Ollama Cloud API...")
    
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        model = os.getenv("OLLAMA_MODEL", "gpt-oss:120b")
        base_url = os.getenv("OLLAMA_BASE_URL", "https://ollama.com")
        
        print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {model}")
        print(f"ğŸŒ æœåŠ¡åœ°å€: {base_url}")
        
        client = ChatOllama(
            model=model,
            base_url=base_url,
            temperature=0.3,
            num_predict=1024,
        )
        
        print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        print("\nğŸ“¤ å‘é€æµ‹è¯•æ¶ˆæ¯: 'å‰è¿›2ç±³'")
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººæŒ‡ä»¤è§£æåŠ©æ‰‹,è¯·å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºJSONæ ¼å¼çš„æŒ‡ä»¤ã€‚"),
            HumanMessage(content="å‰è¿›2ç±³")
        ]
        
        # å¼‚æ­¥è°ƒç”¨
        response = await asyncio.to_thread(client.invoke, messages)
        
        print("âœ… API è°ƒç”¨æˆåŠŸ")
        print(f"\nğŸ“¥ å“åº”å†…å®¹:\n{response.content}\n")
        
        # æµ‹è¯•å¤æ‚æŒ‡ä»¤
        print("ğŸ“¤ å‘é€å¤æ‚æŒ‡ä»¤: 'å¸®æˆ‘æ‹¿æ¡Œä¸Šçš„æ¯å­'")
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººæŒ‡ä»¤è§£æåŠ©æ‰‹,è¯·å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºJSONæ ¼å¼çš„æŒ‡ä»¤åºåˆ—ã€‚"),
            HumanMessage(content="å¸®æˆ‘æ‹¿æ¡Œä¸Šçš„æ¯å­")
        ]
        
        response = await asyncio.to_thread(client.invoke, messages)
        print("âœ… å¤æ‚æŒ‡ä»¤æµ‹è¯•æˆåŠŸ")
        print(f"\nğŸ“¥ å“åº”å†…å®¹:\n{response.content}\n")
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("\næ•…éšœæ’æŸ¥:")
        print("1. æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®è®¤ç½‘ç»œèƒ½è®¿é—® ollama.com")
        print("3. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
        print("4. æŸ¥çœ‹å®Œæ•´é”™è¯¯ä¿¡æ¯è¿›è¡Œè°ƒè¯•")
        return False

if __name__ == "__main__":
    result = asyncio.run(test_ollama())
    sys.exit(0 if result else 1)

